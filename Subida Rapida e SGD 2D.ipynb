{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q3UY-NzWNNBu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class F_x:\n",
        "  def __init__(self, a1= 0, a2= 0, a3= 0, a4= 0, a5= 0, a6= 0, a7= 0, a8= 0, a9= 0, a10= 0, taxaAtualizacao = 0.01, taxaAtualizacaoSGD = 0.01, epsilon = 10**-10, epsilonSGD = 10**-5, precisao= 5, tolerancia = 5):\n",
        "    #Indices da função\n",
        "    self.a1 = a1\n",
        "    self.a2 = a2\n",
        "    self.a3 = a3\n",
        "    self.a4 = a4\n",
        "    self.a5 = a5\n",
        "    self.a6 = a6\n",
        "    self.a7 = a7\n",
        "    self.a8 = a8\n",
        "    self.a9 = a9\n",
        "    self.a10 = a10\n",
        "    #Taxa de atualização inicial\n",
        "    # Taxa de atualização inicial do metódo SGD\n",
        "    #Variavel de tolerancia minima da taxa de atualização, depois de alterada no codigo: epsilon\n",
        "    #Variavel de tolerancia minima do gradiente no metódo SGD, depois de alterada no codigo: epsilonSGD\n",
        "    #uma vez que a taxa de atualização for menor que epsilon o gradiente não será significativamente alterado\n",
        "    #Numero de casas decimais minimas que devem ser iguais dos pontos (x,y) para que a taxa de atualização seja alterada\n",
        "    #Quantidade de vezes que o ponto (x,y) deve ser repetida para de taxa de atualização seja alterada\n",
        "    #Tem que ser >0\n",
        "    #precisao e tolerancia devem ser\n",
        "    if taxaAtualizacao > 0:\n",
        "      self.taxaAtualizacao = taxaAtualizacao\n",
        "    else:\n",
        "      print(\"Taxa de atualização deve ser maior que zero, manteremos ela em seu valor padrão: 0.01\")\n",
        "\n",
        "    if taxaAtualizacaoSGD > 0:\n",
        "      self.taxaAtualizacaoSGD = taxaAtualizacaoSGD\n",
        "    else:\n",
        "      print(\"Taxa de atualização do metodo SGD deve ser maior que zero, manteremos ela em seu valor padrão: 0.01\")\n",
        "\n",
        "    if epsilon > 0:\n",
        "      self.epsilon = epsilon\n",
        "    else:\n",
        "      print(\"Epsilon deve ser maior que zero, manteremos ela em seu valor padrão: 10**-10\")\n",
        "\n",
        "    if epsilonSGD > 0:\n",
        "      self.epsilonSGD = epsilonSGD\n",
        "    else:\n",
        "      print(\"Epsilon SGD deve ser maior que zero, manteremos ela em seu valor padrão: 10**-5\")\n",
        "\n",
        "    if int(precisao) > 0:\n",
        "      self.precisao = int(precisao)\n",
        "    else:\n",
        "      print(\"Precisao deve ser maior que zero, manteremos ela em seu valor padrão: 5. Lembremos que o valor de Precisão deve ser do tipo Integer\")\n",
        "\n",
        "    if int(tolerancia) > 0:\n",
        "      self.tolerancia = int(tolerancia)\n",
        "    else:\n",
        "      print(\"Tolerancia deve ser maior que zero, manteremos ela em seu valor padrão: 5. Lembremos que o valor de Tolerancia deve ser do tipo Integer\")\n",
        "\n",
        "\n",
        "    #Ponto iniciais. Definidos aleatóriamente pela função a cada chamada\n",
        "    x0 = random.random()\n",
        "    y0 = random.random()\n",
        "\n",
        "    print(\"Metodo subida rápida: \" + self.subidaRapida(x0, y0))\n",
        "    print(\"Metodo SGD: \" + self.SGD(x0, y0))\n",
        "\n",
        "\n",
        "\n",
        "  def funcao(self, x, y):\n",
        "    return self.a1*(x**4) + self.a2*(y**4) + self.a3*(x**3) + self.a4*(y**3) + self.a5*(x**2) + self.a6*(y**2) + self.a7*(x) + self.a8*(y) + self.a9*(x*y) + self.a10\n",
        "\n",
        "  def derivadaX(self, x, y):\n",
        "    return self.a1*(4*x**3) + self.a3*(3*x**2) + self.a5*(2*x) + self.a7 + self.a9*y\n",
        "\n",
        "  def derivadaY(self, x, y):\n",
        "    return self.a2*(4*y**3) + self.a4*(3*y**2) + self.a6*(2*y) + self.a8 + self.a9*x\n",
        "\n",
        "  def gradiente(self, x, y):\n",
        "    return np.array([self.derivadaX(x, y), self.derivadaY(x, y)])\n",
        "\n",
        "  def vetorUnitario(self, gradiente):\n",
        "    #Valor da norma do gradiente\n",
        "    norma = np.linalg.norm(gradiente)\n",
        "    return gradiente/norma\n",
        "\n",
        "  #O metodo SGD é um metodo de minimização, logo para aplica-lo em um problema de maximização, devemos espelhar todas funções dependendes de (x,y)\n",
        "  def funcaoEspelhada(self, x, y):\n",
        "    return -self.a1*(x**4) - self.a2*(y**4) - self.a3*(x**3) - self.a4*(y**3) - self.a5*(x**2) - self.a6*(y**2) - self.a7*(x) - self.a8*(y) - self.a9*(x*y) - self.a10\n",
        "\n",
        "  def derivadaXEspelhada(self, x, y):\n",
        "    return -self.a1*(4*x**3) - self.a3*(3*x**2) - self.a5*(2*x) - self.a7 - self.a9*y\n",
        "\n",
        "  def derivadaYEspelhada(self, x, y):\n",
        "    return -self.a2*(4*y**3) - self.a4*(3*y**2) - self.a6*(2*y) - self.a8 - self.a9*x\n",
        "\n",
        "  def gradienteEspelhado(self, x, y):\n",
        "    return np.array([self.derivadaXEspelhada(x, y), self.derivadaYEspelhada(x, y)])\n",
        "\n",
        "\n",
        "  def subidaRapida(self, x, y):\n",
        "    gradiente = self.gradiente(x, y)\n",
        "\n",
        "    #Criamos dois arrays xAtual e yAtual\n",
        "    #Eles armazenam os ultimos 3 valores encontrados de x e y.\n",
        "    #Usado para verificar se a taxa de atualização atual não está fazendo\n",
        "    #a função variar entre os dois mesmos pontos repetidas vezes\n",
        "    #Por exempo (x1,y1) depois de atualizado vão para (x2, y2)\n",
        "    #e (x2, y2) atualizado vai, de volta, para (x1,y1)\n",
        "    xAtual = np.array([])\n",
        "    yAtual = np.array([])\n",
        "    xAtual = np.append(xAtual, x)\n",
        "    yAtual = np.append(yAtual, y)\n",
        "\n",
        "    #Variavel auxiliar count\n",
        "    count = 0\n",
        "\n",
        "    #A fnção ira ser repetida até chegarmos em um resultado aceitavel\n",
        "    repeticao = True\n",
        "    while(repeticao):\n",
        "      '''\n",
        "      print(self.taxaAtualizacao)\n",
        "      print(x)\n",
        "      print(y)\n",
        "      print(self.funcao(x,y))\n",
        "      print(gradiente)\n",
        "      print(\"--------\")\n",
        "      '''\n",
        "      #Se o gradiente for 0 ou proximo o suficiente de 0 achamos o ponto\n",
        "      #se não, atualizamos seguindo a direção do vetor unitário d\n",
        "      if gradiente.all() == 0 or self.taxaAtualizacao < self.epsilon:\n",
        "        repeticao = False\n",
        "        return (\"Ponto de derivada 0 em [%20.20f, %20.20f], com o valor de: %20.20f\"%(x, y, self.funcao(x,y)))\n",
        "      else:\n",
        "        #Encontramos o vetor unitário d\n",
        "        d = self.vetorUnitario(gradiente)\n",
        "        #Atualizamos x e y seguindo o vetor unitário d\n",
        "        x = x+(d[0]*self.taxaAtualizacao)\n",
        "        y = y+(d[1]*self.taxaAtualizacao)\n",
        "        #Armazenamos o ultimo valor achado de x e y\n",
        "        xAtual = np.append(xAtual, x)\n",
        "        yAtual = np.append(yAtual, y)\n",
        "\n",
        "        #Nós so queremos analizar os ultimos 3 valores do ponro (x,y)\n",
        "        #Uma vez que temos o ponto inicial (x1,y1)\n",
        "        #O atualizamos para (x2,y2)\n",
        "        #E atualizamos (x2,y2) para (x3,y3)\n",
        "        #Queremos verificar se o código não está \"quiquando\" sobre os mesmos 2 pontos\n",
        "        #Ou seja queremos saber se (x3,y3) = (x1,y1)\n",
        "        #Logo o vetor so pode ter tamanho 3. Alcançamos isso sempre deletando o primeiro item do array\n",
        "        #sendo que este será, sempre o mais antigo ponto (x,y) analizado\n",
        "        if xAtual.size >= 4 and yAtual.size >= 4:\n",
        "          xAtual = np.delete(xAtual, 0)\n",
        "          yAtual = np.delete(yAtual, 0)\n",
        "\n",
        "          #Queremos então ver se (x3,y3) = (x1,y1) ou proximo o suficiente\n",
        "          #A precisão de casas decimais analisada é definido pela variavel self.precisao\n",
        "          if round(xAtual[0],ndigits=self.precisao) == round(xAtual[2],ndigits=self.precisao) and round(yAtual[0],ndigits=self.precisao) == round(yAtual[2],ndigits=self.precisao):\n",
        "            #A quantidade de vezes que (x3,y3) = (x1,y1) é armazenada em count\n",
        "            count += 1\n",
        "\n",
        "        #Se (x3,y3) = (x1,y1) diversas vezes podemos assumir que a taxa de atualização atual não é mais adequada\n",
        "        #A quantidade que (x3,y3) pode ser igual a (x1,y1) é definida por self.tolerancia\n",
        "        if count>self.tolerancia:\n",
        "          #Atualizamos taxa de atualização ao dividi-la por 10\n",
        "          self.taxaAtualizacao /= 10\n",
        "          count = 0\n",
        "\n",
        "        #Atualizamos o gradiente para o novo ponto (x,y)\n",
        "        gradiente = self.gradiente(x, y)\n",
        "\n",
        "\n",
        "  #Metodo SGD\n",
        "  def SGD(self, x, y):\n",
        "    #Repetimos a função com os pontos atualizados até alcancarmos um ponto satisfatório.\n",
        "    repeticao = True\n",
        "    while(repeticao):\n",
        "      #Achamos o gradiente e o step de atualização pautado pelo gradiente\n",
        "      #Devemos nos lembrar que o gradiente deve ser espelhado já que temos um problema de MAXIMIZAÇÃO\n",
        "      gradiente = self.gradienteEspelhado(x, y)\n",
        "      step = gradiente*self.taxaAtualizacaoSGD\n",
        "\n",
        "      #Se o gradiente for proximo de 0 ou a atualização do step for baixa demais chegamos no nosso resultado aproximado\n",
        "      if gradiente.all() == 0 or (np.absolute(step)[0] <= self.epsilonSGD and np.absolute(step)[1] <= self.epsilonSGD):\n",
        "        repeticao = False\n",
        "        return (\"Ponto de derivada 0 em [%20.20f, %20.20f], com o valor de: %20.20f\"%(x, y, self.funcao(x,y)))\n",
        "      #Se o gradiente for GRANDE além do inverso da nossa tolerancia Epsilon, assumimos que estamos em um problema com ponto de maximo tendendo ao infinito\n",
        "      elif (np.absolute(gradiente)[0] or np.absolute(gradiente)[1]) >= 1/self.epsilonSGD:\n",
        "        repeticao = False\n",
        "        return (\"Ponto de minimo tendendo ao infinito negativo em [%20.20f, %20.20f], com o valor de: %20.20f\"%(x, y, self.funcao(x,y)))\n",
        "      #Quando não estamos em um ponto satisfatório atualizamos o ponto (x,y) diminuindo um valor equivalente a \"step\" dele.\n",
        "      #Definimos assim um novo ponto (x',y') = (x,y) - step com step também sendo um ponto de duas variaveis (\"stepX\", \"stepY\")\n",
        "      else:\n",
        "        x -= step[0]\n",
        "        y -= step[1]\n"
      ],
      "metadata": {
        "id": "TGdm9bwWNYbI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "F_x(a1 = -1,a2 = 1,a3 = 0,a4 = 0,a5 = 0,a6 = -8,a7 = 2,a8 = 2,a9 = 2,a10 = 0)"
      ],
      "metadata": {
        "id": "NrgPaI-tRtB8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cc12a44-fd98-4be7-dec5-e56b20296887"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metodo subida rápida: Ponto de derivada 0 em [0.85147731311400409737, 0.23466528934820071739], com o valor de: 1.60875488164563762439\n",
            "Metodo SGD: Ponto de derivada 0 em [0.85160019578773338278, 0.23470015477613173638], com o valor de: 1.60875481519803376074\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.F_x at 0x7a09a67a79a0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F_x(a1 = -20,a2 = 0,a3 = -1,a4 = 0,a5 = -1,a6 = -6,a7 = 0,a8 = 0,a9 = 20,a10 = 1)"
      ],
      "metadata": {
        "id": "WVTz9TrjNC3s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f757f4e-28b5-4b5b-d26c-6a8022690b00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metodo subida rápida: Ponto de derivada 0 em [0.60736358518345023860, 1.01227262559549080478], com o valor de: 3.83362983104756871455\n",
            "Metodo SGD: Ponto de derivada 0 em [0.60733458496915093683, 1.01214679020777786889], com o valor de: 3.83362976939821642475\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.F_x at 0x7a09a67a6c80>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}
